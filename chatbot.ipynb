{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOemRYtP8vLHawT13BIyBMl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jthaller/chatbot/blob/master/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgkZS4QLvdt2"
      },
      "source": [
        "This chatbot will be loosely based on the [pytorch chatbot tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d1D6gJTwBxh"
      },
      "source": [
        "%%capture\n",
        "!mkdir /datasets\n",
        "!mkdir /datasets/movies\n",
        "!cd /datasets/movies && wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
        "!cd /datasets/movies && unzip -q cornell_movie_dialogs_corpus.zip\n",
        "# upload my facebook chatdata manually. I don't want that data stored anywhere\n",
        "# other than locally, or here since it's temporary."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHCHYHfIMbT3"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "assert tf.__version__.startswith('2')\r\n",
        "tf.random.set_seed(1234)\r\n",
        "\r\n",
        "!pip install tensorflow-datasets==1.2.0\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "\r\n",
        "import os\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfOSuw4UMdMH"
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\r\n",
        "    'cornell_movie_dialogs.zip',\r\n",
        "    origin=\r\n",
        "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\r\n",
        "    extract=True)\r\n",
        "\r\n",
        "path_to_dataset = os.path.join(\r\n",
        "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\r\n",
        "\r\n",
        "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\r\n",
        "path_to_movie_conversations = os.path.join(path_to_dataset,\r\n",
        "                                           'movie_conversations.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m016iFx5MfFQ"
      },
      "source": [
        "# Maximum number of samples to preprocess\r\n",
        "MAX_SAMPLES = 50000\r\n",
        "\r\n",
        "def preprocess_sentence(sentence):\r\n",
        "  sentence = sentence.lower().strip()\r\n",
        "  # creating a space between a word and the punctuation following it\r\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\r\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\r\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\r\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\r\n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\r\n",
        "  sentence = sentence.strip()\r\n",
        "  # adding a start and an end token to the sentence\r\n",
        "  return sentence\r\n",
        "\r\n",
        "\r\n",
        "def load_conversations():\r\n",
        "  # dictionary of line id to text\r\n",
        "  id2line = {}\r\n",
        "  with open(path_to_movie_lines, errors='ignore') as file:\r\n",
        "    lines = file.readlines()\r\n",
        "  for line in lines:\r\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\r\n",
        "    id2line[parts[0]] = parts[4]\r\n",
        "\r\n",
        "  inputs, outputs = [], []\r\n",
        "  with open(path_to_movie_conversations, 'r') as file:\r\n",
        "    lines = file.readlines()\r\n",
        "  for line in lines:\r\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\r\n",
        "    # get conversation in a list of line ID\r\n",
        "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\r\n",
        "    for i in range(len(conversation) - 1):\r\n",
        "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\r\n",
        "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\r\n",
        "      if len(inputs) >= MAX_SAMPLES:\r\n",
        "        return inputs, outputs\r\n",
        "  return inputs, outputs\r\n",
        "\r\n",
        "\r\n",
        "questions, answers = load_conversations()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym5ZsYClMg-p"
      },
      "source": [
        "print('Sample question: {}'.format(questions[20]))\r\n",
        "print('Sample answer: {}'.format(answers[20]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE4st7x4MifI"
      },
      "source": [
        "# Build tokenizer using tfds for both questions and answers\r\n",
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\r\n",
        "    questions + answers, target_vocab_size=2**13)\r\n",
        "\r\n",
        "# Define start and end token to indicate the start and end of a sentence\r\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\r\n",
        "\r\n",
        "# Vocabulary size plus start and end token\r\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLXU3LwrMkTs"
      },
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(questions[20])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geI-SO_fMlkA"
      },
      "source": [
        "# Maximum sentence length\r\n",
        "MAX_LENGTH = 40\r\n",
        "\r\n",
        "\r\n",
        "# Tokenize, filter and pad sentences\r\n",
        "def tokenize_and_filter(inputs, outputs):\r\n",
        "  tokenized_inputs, tokenized_outputs = [], []\r\n",
        "  \r\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\r\n",
        "    # tokenize sentence\r\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\r\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\r\n",
        "    # check tokenized sentence max length\r\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\r\n",
        "      tokenized_inputs.append(sentence1)\r\n",
        "      tokenized_outputs.append(sentence2)\r\n",
        "  \r\n",
        "  # pad tokenized sentences\r\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\r\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\r\n",
        "  \r\n",
        "  return tokenized_inputs, tokenized_outputs\r\n",
        "\r\n",
        "\r\n",
        "questions, answers = tokenize_and_filter(questions, answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUJd7OUtMm_N"
      },
      "source": [
        "print('Vocab size: {}'.format(VOCAB_SIZE))\r\n",
        "print('Number of samples: {}'.format(len(questions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH2PCwE9Ms_u"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "BUFFER_SIZE = 20000\r\n",
        "\r\n",
        "# decoder inputs use the previous target as input\r\n",
        "# remove START_TOKEN from targets\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\r\n",
        "    {\r\n",
        "        'inputs': questions,\r\n",
        "        'dec_inputs': answers[:, :-1]\r\n",
        "    },\r\n",
        "    {\r\n",
        "        'outputs': answers[:, 1:]\r\n",
        "    },\r\n",
        "))\r\n",
        "\r\n",
        "dataset = dataset.cache()\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\r\n",
        "dataset = dataset.batch(BATCH_SIZE)\r\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TjpsN1QMwgk"
      },
      "source": [
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}